{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sprint 12 - SimpleConv2d.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiEWmeDEax42"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import sys,os\n",
        "sys.path.append(r\"C:\\Users\\anai\\dive\\oreilly\\deep-learning-from-scratch\\common\")\n",
        "from utils import im2col, col2im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfApu9K5bVu-"
      },
      "source": [
        "sys.path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-GRb3g0acPm"
      },
      "source": [
        "# 【Problem 1】Creating a 2-D convolutional layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWGPdxfIbNrn"
      },
      "source": [
        "Develop the class Conv1d of 1D convolutional layers and create the class Conv2d of 2D convolutional layers.\n",
        "\n",
        "The formula for forward propagation is as follows\n",
        "$$a_{i, j, m} = \\sum_{k=0}^{K-1} \\sum_{s=0}^{F_h -1}\\sum_{t=0}^{F_w -1} x_{(i+s),(j+t),k}w_{s,t,k,m} + b_m$$.\n",
        "\n",
        "$a_{i,j,m}$ : value of row i, column j and channel m of the output array\n",
        "\n",
        "$i$ : index of the array in the row direction\n",
        "\n",
        "$j$ : Array column index\n",
        "\n",
        "$m$ : index of the output channel\n",
        "\n",
        "$K$ : Number of input channels\n",
        "\n",
        "$F_h,F_w$ : Size of the filter in height (h) and width (w) direction\n",
        "\n",
        "$x_{(i+s),(j+t),k}$ : (i+s)-row (j+t)-column, k-channel value of the input array\n",
        "\n",
        "$w_{s,t,k,m}$ : row s, column t of the array of weights, for k-channel input, output weights for m-channel\n",
        "\n",
        "$b_m$ : bias term of the output to the m-channel.\n",
        "\n",
        "All are scalars.\n",
        "\n",
        "Next is the update formula, which has the same form as for the 1D convolutional and all-combining layers.\n",
        "$$w'_{s, t, k, m} = w_{s, t, k, m} - \\alpha \\frac{partial L}{\\partial w_{s, t, k, m}}}$$\n",
        "\n",
        "$$b'_m = b_m - \\alpha \\frac{\\partial L}{\\partial b_m}$$\n",
        "\n",
        "$\\alpha$ : Learning rate\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w_{s, t, k, m}}$: Gradient of loss $L$ with respect to $w_{s,t,k,m}$.\n",
        "\n",
        "$\\frac{\\partial L}{\\partial b_m}$ : Gradient of loss $L$ with respect to $b_m$.\n",
        "\n",
        "The back-propagation formulae for finding the gradient $\\frac{\\partial L}{\\partial w_{s, t, k, m}}$ and $\\frac{\\partial L}{\\partial b_m}$ are as follows\n",
        "$$\\frac{\\partial L}{\\partial w_{s, t, k, m}} = \\sum_{i=0}^{N_{out, h}-1} \\sum_{j=0}^{N_{out, w}-1} \\frac{\\partial L}{\\partial a_{i,j,m}} x_{(i+s )(j+k),k}$$$$\\frac{\\partial L}{\\partial b_m} = sum_{i=0}^{N_{out, h}-1} sum_{j=0}^{N_{out, w}-1}\\frac{\\partial L}{\\partial a_{i,j,m}}$$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial a_{i,j,m}}$ : The value of the i-th row, j-th column and m-channel of the gradient array\n",
        "\n",
        "$N_{out,h},N_{out,w}$ : Size of the output in height direction (h) and width direction (w)\n",
        "\n",
        "The formula for the error to be passed to the previous layer is as follows\n",
        "$$\\frac{\\partial L}{\\partial x_{i,j,k}} = \\sum_{m=0}^{M-1} \\sum_{s=0}^{F_{h-1}} \\sum_{t=0}^{F_{w-1}} \\frac{\\partial L}{\\partial a_{(i-s)(j-t),m}}w_{s, t, k, m}$$.\n",
        "\n",
        "$\\frac{\\partial L}{\\partial x_{i,j,k}}$ : i-column, j-row, k-channel value of the array of errors to be passed to the previous layer\n",
        "\n",
        "$M$ : Number of output channels\n",
        "However, when $i-s&lt;0$ or $i-s&gt;N_{out,h}-1$ or $j-t&lt;0$ or $j-t&gt;N_{out,w }-1$ when\n",
        "$\\frac{\\partial L}{\\partial a_{(i-s)(j-t),m}}w_{s, t, k, m}=0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu345RGNa4R_"
      },
      "source": [
        "# [Problem 2] Experiments with 2D convolutional layers on small arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jszUQFOa_NV"
      },
      "source": [
        "The convolution changes the size of the feature map. The formula below will tell you how it changes. Create a function to perform this calculation.\n",
        "$$N_{h, out} = \\frac{N_{h, in} + 2P_h - F_h}{S_h} + 1$$$$N_{w, out} = \\frac{N_{w, in} + 2P_w - F_w}{S_w} + 1$$.\n",
        "\n",
        "$N_{out}$ : size of the output (number of features)\n",
        "\n",
        "$N_{in}$ : size of the input (number of features)\n",
        "\n",
        "$P$ : Number of paddings in a direction\n",
        "\n",
        "$F$ : size of the filter\n",
        "\n",
        "$S$ : size of the stride\n",
        "\n",
        "where $H$ is the height direction and $W$ is the width direction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qavR1xoIbuCm"
      },
      "source": [
        "class Conv2d:\n",
        "    \"\"\"\n",
        "    All coupling layers from n_nodes1 to n_nodes2\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the next layer\n",
        "    initializer : instance of initialization method\n",
        "    optimizer : instance of optimisation method\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.P = 0\n",
        "        self.Str = 1\n",
        "        self.a = np.array([])\n",
        "        self.dW = np.array([])\n",
        "        self.dX = np.array([])\n",
        "        #self.s=None\n",
        "        \n",
        "    def forward(self, X, W ,B):\n",
        "        \"\"\"\n",
        "        Forward\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray of the following form, shape (batch_size, n_nodes_bf)\n",
        "            Input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : ndarray of the following form, shape (batch_size, n_nodes_af)\n",
        "            Outputs\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.XN, self.XC, self.XH, self.XW = self.X.shape\n",
        "        \n",
        "        self.W = W \n",
        "        self.FN, self.FC, self.FH, self.FW = self.W.shape\n",
        "        self.B = B\n",
        "        self.s = self.W.shape[3]\n",
        "        \n",
        "        self._output_size()\n",
        "        \n",
        "        self.Wre = self.W.reshape(self.FN,-1)\n",
        "        #self.Bre = np.array([self.B] * self.Nhout * self.Nwout).reshape(-1,1)\n",
        "        self.Bre = self.B.reshape(1,-1)\n",
        "        self.col = im2col(self.X , self.FH, self.FW, self.Str, self.P)\n",
        "        \n",
        "        display(self.col)\n",
        "        display(self.Wre.T)\n",
        "        #display(self.Bre)\n",
        "        display(self.col @ self.Wre.T)\n",
        "        self.Are = (self.col @ self.Wre.T) + self.Bre\n",
        "        display(self.Are)\n",
        "        self.a = self.Are.reshape(self.XN, self.Nhout, self.Nwout,-1).transpose(0,3,1,2)\n",
        "        display(self.a.shape)\n",
        "        return self.a\n",
        "    \n",
        "    def _output_size(self):\n",
        "        self.Nhout = int((self.XH + 2*self.P - self.FH) / self.Str + 1)       \n",
        "        self.Nwout = int((self.XW + 2*self.P - self.FW) / self.Str + 1)\n",
        "        \n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
        "            The gradient flowed from behind\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : ndarray of the next shape, shape (batch_size, n_nodes1)\n",
        "            Gradient flowing forward\n",
        "        \"\"\"\n",
        "        self.dA=dA.transpose(0,2,3,1).reshape(-1,self.FN)\n",
        "        self.dB = np.sum(self.dA, axis=0)\n",
        "        \n",
        "        display(self.dA)\n",
        "        display(self.col.T)\n",
        "        self.dW = np.dot(self.col.T , self.dA)\n",
        "        display(self.dW)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(self.FN, self.FC, self.FH, self.FW)\n",
        "        display(self.dW)\n",
        "        \n",
        "        self.dXre = np.dot(self.dA, self.Wre) \n",
        "        self.dX = col2im(self.dXre, self.X.shape, self.FH, self.FW, self.Str, self.P)\n",
        "\n",
        "        return self.dX"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gJcCe0ob0FP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8add864c-0e16-4488-805e-9a78a36841d5"
      },
      "source": [
        "x = np.array([[2, 3, 4, 5], [1, 2, 3, 4]]) # shape(2, 4), where (number of input channels, number of features).\n",
        "w = np.array([[[[1,1,1],\n",
        "            [1,1,1]],\n",
        "            [[1,1,1],\n",
        "            [2,1,1]],\n",
        "            [[2,1,1],\n",
        "            [1,1,2]]]])\n",
        "w=w.transpose(1,0,2,3)\n",
        "display(w.shape)\n",
        "b = np.array([3, 2, 1]) # (Number of output channels)\n",
        "b=np.array([b]*1)\n",
        "x = np.array([[x]*1]*1)\n",
        "x.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(3, 1, 2, 3)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 2, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2B3vQKQb2JC"
      },
      "source": [
        "dnn2 = Conv2d()\n",
        "dnn2.forward(x,w,b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwJvw7O1b4SN"
      },
      "source": [
        "dA = np.array([[[52,56]],\n",
        "            [[32,35]],\n",
        "            [[9,11]]])\n",
        "dA = np.array([dA]*1)\n",
        "dA.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjVQfN3xb6FZ"
      },
      "source": [
        "dnn2.backward(dA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3RTJRFkb8iM"
      },
      "source": [
        "ten_2 = np.array([[[1., 2., 3., 4.],\n",
        "                 [1., 2., 3., 4.]],\n",
        "                [[2., 3., 4., 5.],\n",
        "                 [2., 3., 4., 5.]]])\n",
        "\n",
        "ten_2 = np.expand_dims(ten_2, 0)\n",
        "print('ten_2.shape{}'.format(ten_2.shape))\n",
        "print('ten_2{}'.format(ten_2)) # (1, 2, 2, 4)\n",
        "\n",
        "kernel = np.array([[[[1., 2., 1.],\n",
        "                    [1., 1., 1.],\n",
        "                    [2., 1., 1.]]],\n",
        "                   [[[2., 1., 1.],\n",
        "                    [1., 1., 1.],\n",
        "                    [1., 1., 1.]]]])\n",
        "\n",
        "kernel = kernel.transpose(3,0,1,2)\n",
        "print('kernel.shape{}'.format(kernel.shape))\n",
        "print('kernel{}'.format(kernel))\n",
        "\n",
        "bias = np.array([[1],[2],[3]])\n",
        "print('bias.shape{}'.format(bias.shape))\n",
        "\n",
        "print('bias{}'.format(bias))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-sfhL3hb-7L"
      },
      "source": [
        "dnn3 = Conv2d()\n",
        "dnn3.forward(ten_2,kernel,bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uomUG28XcBQt"
      },
      "source": [
        "loss_2 = np.array([[[[9., 11.],\n",
        "                   [9., 11.]],\n",
        "                  [[32., 35.],\n",
        "                   [32., 35.]],\n",
        "                  [[52., 56.],\n",
        "                   [52., 56.]]]])\n",
        "display(loss_2.shape)\n",
        "dnn3.backward(loss_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-JVRNXicHaF"
      },
      "source": [
        "# [Problem 3] Output size after 2-dimensional convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr4oR_FVcfFR"
      },
      "source": [
        "$$a_{i,j,k} = max_{(p,q)\\in P_{i,j}} x_{p,q,k}$$.\n",
        "\n",
        "$P_{i,j}$ : the set of indices of the input array when outputting to row i and column j. Rows $(p)$ and columns $(q)$ in the range $S_h×S_w$.\n",
        "\n",
        "$S_h,S_w$ : size of stride in height direction $(h)$ and width direction $(w)$.\n",
        "\n",
        "$(p,q)\\in P_{i,j}$ : Index of the row $(p)$ and column $(q)$ in $P_{i,j}$.\n",
        "\n",
        "$a_{i,j,m}$ : value of row i, column j and channel k of the output array\n",
        "\n",
        "$x_{p,q,k}$ : $p$-row $q$-column, $k$-channel values of the input array\n",
        "\n",
        "Within a certain range, the maximum value will be calculated while leaving the axes in the channel direction.\n",
        "\n",
        "For back-propagation we need to keep the index $(p,q)$ of the maximum value at the time of forward-propagation. The reason for this is that we want to keep the same error in the forward-propagated maximum, and zero in the other parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntO7RkcecQNp"
      },
      "source": [
        "class Pooling:\n",
        "    \"\"\"\n",
        "    All coupling layers from n_nodes1 to n_nodes2\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the next layer\n",
        "    initializer : instance of initialization method\n",
        "    optimizer : instance of optimisation method\n",
        "    \"\"\"\n",
        "    def __init__(self, PH ,PW):\n",
        "        self.P = 0\n",
        "        self.Str = 2\n",
        "        self.PH = PH\n",
        "        self.PW = PW\n",
        "        \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray of the following form, shape (batch_size, n_nodes_bf)\n",
        "            Input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : ndarray of the following form, shape (batch_size, n_nodes_af)\n",
        "            Outputs\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.XN, self.XC, self.XH, self.XW = self.X.shape\n",
        "        #self.X = self.X.transpose(1,0,2,3)\n",
        "        self.Xre = self.X.reshape(-1, 1, self.XH, self.XW)\n",
        "        self.pcol = im2col(self.Xre , self.PH, self.PW, self.Str, self.P)\n",
        "        #display(self.pcol)\n",
        "        self.p_index = np.argmax(self.pcol, axis=1)\n",
        "        #display(self.p_index)\n",
        "        self.p_out_re =np.max(self.pcol,axis=1)\n",
        "        #display(self.p_out_re)\n",
        "        self._out_psize()\n",
        "        self.p_out = self.p_out_re.reshape(self.XN, self.Phout, self.Pwout,-1).transpose(0,3,1,2)\n",
        "        self.p_out = self.p_out_re.reshape(self.XN, -1, self.Phout, self.Pwout)\n",
        "        #display(self.pcol)\n",
        "        \n",
        "        return self.p_out\n",
        "    \n",
        "    def _out_psize(self):\n",
        "        self.Phout = int((self.XH + 2*self.P - self.PH) / self.Str + 1)       \n",
        "        self.Pwout = int((self.XW + 2*self.P - self.PW) / self.Str + 1)\n",
        "    \n",
        "    def backward(self, dPin):\n",
        "        \"\"\"\n",
        "        Backward\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
        "            The gradient flowed from behind\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : ndarray of the next shape, shape (batch_size, n_nodes1)\n",
        "            Gradient flowing forward\n",
        "        \"\"\"\n",
        "        self.dPre = dPin.reshape(1,-1)\n",
        "        self.backP = np.zeros((self.pcol.shape)) \n",
        "        for i in range(len(self.p_index)):\n",
        "            self.backP[i][self.p_index[i]] = self.dPre[:,i]\n",
        "        #display(self.backP)\n",
        "        #display(self.pcol)\n",
        "        self.dbackP = col2im(self.backP, self.Xre.shape, self.PH, self.PW, self.Str, self.P)\n",
        "        self.dbackP = self.dbackP.reshape(self.X.shape)\n",
        "        return self.dbackP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl6LZZ0kcULD"
      },
      "source": [
        "xin = np.array([[[[1,3,2,9],\n",
        "                  [7,4,1,5],\n",
        "                  [8,5,2,3],\n",
        "                  [4,2,1,4]],\n",
        "                 \n",
        "                 [[1,3,2,9],\n",
        "                  [7,4,1,5],\n",
        "                  [8,5,2,3],\n",
        "                  [4,2,1,4]]],\n",
        "               [[[1,3,2,9],\n",
        "                  [7,4,1,5],\n",
        "                  [8,5,2,3],\n",
        "                  [4,2,1,4]],\n",
        "                 \n",
        "                 [[1,3,2,9],\n",
        "                  [7,4,1,5],\n",
        "                  [8,5,2,3],\n",
        "                  [4,2,1,4]]]])\n",
        "display(xin.shape)\n",
        "scr_pool=Pooling(2,2)\n",
        "scr_pool.forward(xin).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie3OnSGecW-T"
      },
      "source": [
        "#dP = np.array([[[[1,2,3],\n",
        "#                 [4,5,6],\n",
        "#                 [7,8,9]]]])\n",
        "dP = np.array([[[[1,2],\n",
        "                 [3,4]],\n",
        "                \n",
        "                [[1,2],\n",
        "                 [3,4]]],\n",
        "              [[[1,2],\n",
        "                 [3,4]],\n",
        "                \n",
        "                [[1,2],\n",
        "                 [3,4]]]])\n",
        "display(dP.shape)\n",
        "scr_pool.backward(dP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk25P1pkcvdY"
      },
      "source": [
        "# [Problem 5] (Advance task) Creating average pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcQUWLQ8c0g1"
      },
      "source": [
        "class Flatten:\n",
        "    \"\"\"\n",
        "    All join layers from number of nodes n_nodes1 to n_nodes2\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the next layer\n",
        "    initializer : instance of initialization method\n",
        "    optimizer : instance of the optimisation method\n",
        "    \"\"\"\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        forward(self, X)\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray of the following form, shape (batch_size, n_nodes_bf)\n",
        "            Input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : ndarray of the following form, shape (batch_size, n_nodes_af)\n",
        "            Outputs\n",
        "        \"\"\"\n",
        "        self.XN, self.XC, self.XH, self.XW = X.shape\n",
        "        self.flatout = X.reshape(self.XN,-1)\n",
        "        \n",
        "        return self.flatout\n",
        "    \n",
        "    def backward(self, dPin):\n",
        "        \"\"\"\n",
        "        Backward\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
        "            The gradient flowed from behind\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : ndarray of the next shape, shape (batch_size, n_nodes1)\n",
        "            Gradient flowing forward\n",
        "        \"\"\"\n",
        "        self.dFlatten = dPin.reshape(self.XN, self.XC, self.XH, self.XW )\n",
        "        \n",
        "        return self.dFlatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIxAMHSrc39W"
      },
      "source": [
        "scr_flatten = Flatten()\n",
        "scr_flatten.forward(dnn2.forward(x,w,b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwPtCoZkc6vj"
      },
      "source": [
        "deluta_flat = np.array([1,2,3,4,5,6])\n",
        "scr_flatten.backward(deluta_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl9YyfJlc98Q"
      },
      "source": [
        "# [Problem 6] Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbPhp0qydCDJ"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go21gDZzdEnW"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjayztJ8dHo9"
      },
      "source": [
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "X_train = X_train[:, np.newaxis,:,:]\n",
        "X_test = X_test[:, np.newaxis,:,:]\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCTYRVnndL8f"
      },
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to retrieve the mini-batch\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray of the following form, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : ndarray of the following form, shape (n_samples, 1)\n",
        "      The correct answer value\n",
        "    batch_size : int\n",
        "      batch size\n",
        "    seed : int\n",
        "      Seed of random number in NumPy\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM8Krp51dPMh"
      },
      "source": [
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    Simple initialization with Gaussian distribution\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      Standard deviation of the Gaussian distribution\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "        #display(self.sigma.Calc)\n",
        "    def W(self, FC, FN, FH=3, FW=3):\n",
        "        \"\"\"\n",
        "        Initialize the weights\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the next layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W :\n",
        "        \"\"\"\n",
        "        W = self.sigma * np.random.randn(FN, FC, FH, FW)\n",
        "        return W\n",
        "    \n",
        "    def B(self, FN):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in the next layer\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B :\n",
        "        \"\"\"\n",
        "        B = self.sigma * np.random.randn(1, FN)\n",
        "        return B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijhCuZXRdSON"
      },
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic gradient descent method\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, dWorB, WorB):\n",
        "        \"\"\"\n",
        "        Update the weights and biases of a layer\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : the instance of the layer before the update\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        layer : the instance of the layer after the update\n",
        "        \"\"\"\n",
        "        self.WorB = WorB\n",
        "        self.WorB -= self.lr*dWorB\n",
        "        return self.WorB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgDGKfv1dWdX"
      },
      "source": [
        "class Activation:\n",
        "    \"\"\"\n",
        "    Activation function tanh\n",
        "    Parameters\n",
        "    ----------\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def tanh_fw(self, X):\n",
        "        \"\"\"\n",
        "        Forward\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray of the following form, shape (batch_size, n_nodes_bf)\n",
        "            Input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : ndarray of the following form, shape (batch_size, n_nodes_af)\n",
        "            Outputs\n",
        "        \"\"\"     \n",
        "        self.A = X\n",
        "        Z = np.tanh(X)\n",
        "        return Z\n",
        "\n",
        "    def tanh_bw(self, dZ):\n",
        "        \"\"\"\n",
        "        Backward\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
        "            The gradient flowed from behind\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : ndarray of the next shape, shape (batch_size, n_nodes1)\n",
        "            Gradient flowing forward\n",
        "        \"\"\"\n",
        "        dA = dZ * (1 - np.tanh(self.A)**2)  \n",
        "        return dA\n",
        "    \n",
        "    def softmax_fw(self, X):\n",
        "        \"\"\"\n",
        "        Forward\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray of the following form, shape (batch_size, n_nodes_bf)\n",
        "            Input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : ndarray of the following form, shape (batch_size, n_nodes_af)\n",
        "            Outputs\n",
        "        \"\"\"     \n",
        "        Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
        "        return Z\n",
        "\n",
        "    def softmax_bw(self, Z, y):\n",
        "        \"\"\"\n",
        "        Backward\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
        "            The gradient flowed from behind\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : ndarray of the next shape, shape (batch_size, n_nodes1)\n",
        "            Gradient flowing forward\n",
        "        \"\"\"\n",
        "        dA = Z - y\n",
        "        return dA\n",
        "\n",
        "    def entropy(self, Z, y):\n",
        "        \"\"\"\n",
        "        Forward\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray of the following form, shape (batch_size, n_nodes_bf)\n",
        "            Input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : ndarray of the following form, shape (batch_size, n_nodes_af)\n",
        "            Outputs\n",
        "        \"\"\"     \n",
        "        L = -1*np.average(np.sum(y * np.log(Z), axis=1), axis=0)\n",
        "        return L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nypRsXXudaE7"
      },
      "source": [
        "class Xavier:\n",
        "    \"\"\"\n",
        "    Activation function tanh\n",
        "    Parameters\n",
        "    ----------\n",
        "    \"\"\"\n",
        "    def __init__(self,n):\n",
        "        self.n=n\n",
        "        self.calc = 1/np.sqrt(self.n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ii8yY2vdcyw"
      },
      "source": [
        "class FC:\n",
        "    \"\"\"\n",
        "    All coupling layers from n_nodes1 to n_nodes2\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the next layer\n",
        "    initializer : instance of initialization method\n",
        "    optimizer : instance of optimisation method\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, sigma, optimizer):\n",
        "        # Initialization\n",
        "        # Use the initializer method to initialize self.W and self.B\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.optimizer = optimizer\n",
        "        #self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "        #self.B = initializer.B(self.n_nodes2)\n",
        "        display(sigma)\n",
        "        self.W = sigma * np.random.randn(self.n_nodes1, self.n_nodes2)\n",
        "        self.B = sigma * np.random.randn(1, self.n_nodes2)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray of the following form, shape (batch_size, n_nodes_bf)\n",
        "            Input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : ndarray of the following form, shape (batch_size, n_nodes_af)\n",
        "            Outputs\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        #display(self.X.shape)\n",
        "        #display(self.W.shape)\n",
        "        #display(self.B.shape)\n",
        "        A = np.dot(self.X, self.W) +  self.B\n",
        "        #display(A.shape)\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
        "            The gradient flowed from behind\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : ndarray of the next shape, shape (batch_size, n_nodes1)\n",
        "            Gradient flowing forward\n",
        "        \"\"\"\n",
        "        dB = np.sum(dA, axis=0)\n",
        "        dW = np.dot(self.X.T, dA)\n",
        "        dZ = np.dot(dA, self.W.T)     \n",
        "\n",
        "        # Updates\n",
        "        #self = self.optimizer.update(dW, dB, self.W, self.W)\n",
        "        self.W = self.optimizer.update(dW, self.W)\n",
        "        self.B = self.optimizer.update(dB, self.B)\n",
        "\n",
        "        return dZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Y45Ky9dfzc"
      },
      "source": [
        "class Conv2d:\n",
        "    \"\"\"\n",
        "    All coupling layers from n_nodes1 to n_nodes2\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in the next layer\n",
        "    initializer : instance of initialization method\n",
        "    optimizer : instance of the optimisation method\n",
        "    \"\"\"\n",
        "    def __init__(self, initializer, optimizer, FC, FN, FH=3, FW=3):\n",
        "        self.P = 0\n",
        "        self.Str = 1\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(FC, FN)\n",
        "        self.B = initializer.B(FN)\n",
        "\n",
        "        self.a = np.array([])\n",
        "        self.dW = np.array([])\n",
        "        self.dX = np.array([])\n",
        "        #self.s=None\n",
        "        \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray of the following form, shape (batch_size, n_nodes_bf)\n",
        "            Input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : ndarray of the following form, shape (batch_size, n_nodes_af)\n",
        "            Outputs\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.XN, self.XC, self.XH, self.XW = self.X.shape\n",
        "        self.FN, self.FC, self.FH, self.FW = self.W.shape\n",
        "        self.s = self.W.shape[3]\n",
        "        \n",
        "        self._output_size()\n",
        "        \n",
        "        self.Wre = self.W.reshape(self.FN,-1)\n",
        "        self.Bre = self.B.reshape(1,-1)\n",
        "        self.col = im2col(self.X , self.FH, self.FW, self.Str, self.P)\n",
        "        \n",
        "        self.Are = (self.col @ self.Wre.T) + self.Bre\n",
        "        self.a = self.Are.reshape(self.XN, self.Nhout, self.Nwout,-1).transpose(0,3,1,2)\n",
        "\n",
        "        return self.a\n",
        "    \n",
        "    def _output_size(self):\n",
        "        self.Nhout = int((self.XH + 2*self.P - self.FH) / self.Str + 1)       \n",
        "        self.Nwout = int((self.XW + 2*self.P - self.FW) / self.Str + 1)\n",
        "        \n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
        "            The gradient flowed from behind\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : ndarray of the next shape, shape (batch_size, n_nodes1)\n",
        "            Gradient flowing forward\n",
        "        \"\"\"\n",
        "        self.dA=dA.transpose(0,2,3,1).reshape(-1,self.FN)\n",
        "        self.dB = np.sum(self.dA, axis=0)\n",
        "        \n",
        "        self.dW = np.dot(self.col.T , self.dA)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(self.FN, self.FC, self.FH, self.FW)\n",
        "        \n",
        "        self.dXre = np.dot(self.dA, self.Wre) \n",
        "        self.dX = col2im(self.dXre, self.X.shape, self.FH, self.FW, self.Str, self.P)\n",
        "\n",
        "        self.W = self.optimizer.update(self.dW, self.W)\n",
        "        self.B = self.optimizer.update(self.dB, self.B)\n",
        "        \n",
        "        return self.dX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FULhGtPedsRj"
      },
      "source": [
        "class ScratchSimpleNeuralNetrowkClassifier():\n",
        "    \"\"\"\n",
        "    A simple three-layer neural network classifier\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bp : int\n",
        "        Number of backpropagations\n",
        "    Attributes\n",
        "    ----------\n",
        "    \"\"\"\n",
        "    def __init__(self, how_prp=\"htan\", FN1=3, FN2=6, n_output=10, n_epoq=5, batch_size=20, \n",
        "                 lr=0.001, verbose = True):\n",
        "        self.n_epoq = n_epoq\n",
        "        self.FN1 = FN1\n",
        "        self.FN2 = FN2\n",
        "        self.n_output = n_output\n",
        "        self.batch_size = batch_size\n",
        "        self.how_prp = how_prp\n",
        "        self.lr = lr\n",
        "        self.loss_list = np.array([])\n",
        "        self.loss_val_list = np.array([])\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Trains a neural network classifier.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray of the following form, shape (n_samples, n_features)\n",
        "            Features of the training data.\n",
        "        y : ndarray of the following form, shape (n_samples, )\n",
        "            The correct answer value of the training data\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.XN, self.XC, self.XH, self.XW = self.X.shape\n",
        "        \n",
        "        optimizer = SGD(self.lr)\n",
        "        sigma1=Xavier(self.XC*3*3)\n",
        "        sigma2=Xavier(self.FN1*3*3)\n",
        "        \n",
        "        flatten_node = self.FN2*5*5\n",
        "        #last_node = np.round(flatten_node/2).astype(int)\n",
        "        sigma3=Xavier(self.n_output)\n",
        "        \n",
        "        self.CONV1 = Conv2d(SimpleInitializer(sigma1.calc), optimizer, self.XC, self.FN1, FH=3, FW=3)\n",
        "        self.activation1 = Activation()\n",
        "        self.MaxPool1 = Pooling(PH=2,PW=2)\n",
        "        self.CONV2 = Conv2d(SimpleInitializer(sigma2.calc), optimizer, self.FN1, self.FN2, FH=3, FW=3)\n",
        "        self.activation2 = Activation()\n",
        "        self.MaxPool2 = Pooling(PH=2,PW=2)\n",
        "        self.FLAT = Flatten()\n",
        "        self.DENSE = FC(flatten_node, self.n_output, sigma3.calc, optimizer)\n",
        "        self.activation3 = Activation()\n",
        "        \n",
        "        self.val = 0\n",
        "        get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
        "        for _ in range(self.n_epoq):\n",
        "            for X_mini, y_mini in get_mini_batch:\n",
        "                self.X_ = X_mini\n",
        "                self.y_ = y_mini\n",
        "                #display(sigma3)\n",
        "                self._forward_propagation(self.X_)\n",
        "                self._back_propagation()\n",
        "                \n",
        "            self.L = self.activation3.entropy(self.Z3, self.y_)        \n",
        "            self.loss_list = np.append(self.loss_list, self.L)\n",
        "        \n",
        "    def _forward_propagation(self, X):\n",
        "        self.a1 = self.CONV1.forward(X)\n",
        "        self.Z1 = self.activation1.tanh_fw(self.a1)\n",
        "        #display(self.Z1.shape)\n",
        "        self.p1 = self.MaxPool1.forward(self.Z1)\n",
        "        #display(self.p1.shape)\n",
        "        self.a2 = self.CONV2.forward(self.p1)\n",
        "        #display(self.a2.shape)\n",
        "        self.Z2 = self.activation2.tanh_fw(self.a2)\n",
        "        #display(self.Z2.shape)\n",
        "        self.p2 = self.MaxPool2.forward(self.Z2)\n",
        "        #display(self.p2.shape)\n",
        "        self.flat = self.FLAT.forward(self.p2) \n",
        "        #display(self.flat.shape)\n",
        "        \n",
        "        self.den = self.DENSE.forward(self.flat)\n",
        "        #display(self.den.shape)\n",
        "        self.Z3 = self.activation3.softmax_fw(self.den)\n",
        "        #display(self.Z3.shape)\n",
        "        \n",
        "    def _back_propagation(self):\n",
        "        dA3 = self.activation3.softmax_bw(self.Z3, self.y_)\n",
        "        #display(dA3.shape)\n",
        "        dZ2 = self.DENSE.backward(dA3)\n",
        "        #display(dZ2.shape)\n",
        "        dflat = self.FLAT.backward(dZ2)\n",
        "        #display(dflat.shape)\n",
        "        dP2 = self.MaxPool2.backward(dflat)\n",
        "        #display(dP2.shape)\n",
        "        dA2 = self.activation2.tanh_bw(dP2)\n",
        "        dZ1 = self.CONV2.backward(dA2)\n",
        "        \n",
        "        dP1 = self.MaxPool1.backward(dZ1)\n",
        "        dA1 = self.activation1.tanh_bw(dP1)\n",
        "        #dZ0 = self.CONV2.backward(dA1)        \n",
        "\n",
        "    def graph_cost_func(self):\n",
        "        \"\"\"\n",
        "        Graph the loss trend.    \n",
        "        If the data for verification is input, the loss trends for training and verification are superimposed and graphed.\n",
        "        \"\"\"\n",
        "        plt.title(\"Num_of_Epoq vs Loss\")\n",
        "        plt.xlabel(\"Num_of_Epoq\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.plot(range(1,self.n_epoq+1), self.loss_list, color=\"b\", marker=\"o\", label=\"train_loss\")\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "  \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimation using a neural network classifier.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray of the following form, shape (n_samples, n_features)\n",
        "            Samples\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "            ndarray of the following form, shape (n_samples, 1)\n",
        "            Estimation results\n",
        "        \"\"\"\n",
        "        #a1 = self.CONV1.forward(X)\n",
        "        #Z1 = self.activation1.tanh_fw(a1)\n",
        "        ##display(self.Z1.shape)\n",
        "        #p1 = self.MaxPool1.forward(Z1)\n",
        "        ##display(self.p1.shape)\n",
        "        #a2 = self.CONV2.forward(p1)\n",
        "        ##display(self.a2.shape)\n",
        "        #Z2 = self.activation2.tanh_fw(a2)\n",
        "        ##display(self.Z2.shape)\n",
        "        #p2 = self.MaxPool2.forward(Z2)\n",
        "        ##display(self.p2.shape)\n",
        "        #flat = self.FLAT.forward(p2) \n",
        "        ##display(self.flat.shape)\n",
        "        #\n",
        "        #den = self.DENSE.forward(flat)\n",
        "        ##display(self.den.shape)\n",
        "        #Z3 = self.activation3.softmax_fw(den)\n",
        "        \n",
        "        self._forward_propagation(X)\n",
        "        #display(Z3)\n",
        "        return np.argmax(self.Z3, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujilMZvKd4Gk"
      },
      "source": [
        "scr_nnc = ScratchSimpleNeuralNetrowkClassifier(batch_size=100)\n",
        "scr_nnc.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0k9moMEd6QW"
      },
      "source": [
        "scr_nnc.predict(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_3UO7mtd8jb"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(\"accuracy:{}\".format(accuracy_score(y_test, scr_nnc.predict(X_test))))\n",
        "print(\" {}\".format(confusion_matrix(y_test, scr_nnc.predict(X_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a143-GPKd-4I"
      },
      "source": [
        "scr_nnc.graph_cost_func()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QCDNrRNeEPU"
      },
      "source": [
        "# [Problem 9] (Advance assignment) Survey of famous image recognition models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSY1GpiPeMC0"
      },
      "source": [
        "When building a CNN model, it is necessary to calculate in advance the number of features at the stage of input to the all-connected layer. In addition, when dealing with large models, the calculation of the number of parameters becomes a necessity due to memory and computation speed. The framework can show you the number of parameters for each layer, but you need to understand the meaning to be able to adjust it properly.\n",
        "\n",
        "Calculate the output size and the number of parameters for the following three convolutional layers. For the number of parameters also consider the bias term.\n",
        "\n",
        "1. input size : 144×144, 3 channels filter size : 3×3, 6 channels stride : 1 padding : none\n",
        "\n",
        "2. input size : 60 x 60, 24 channels filter size : 3 x 3, 48 channels stride : 1 padding : none\n",
        "\n",
        "3. input size : 20x20, 10 channels filter size : 3x3, 20 channels stride : 2 padding : none\n",
        "\n",
        "The last example is a case where the convolution cannot be done just right. The last example is a case where the convolution can't be done just right, the framework may not see the extra pixels. This is an example of why such a setting is undesirable, as it will result in missing edges.\n",
        "\n",
        "1. input size : 144×144, 3 channels filter size : 3×3, 6 channels stride : 1 padding : none\n",
        "\n",
        "Answer Output size = 141 x 141 Number of parameters = 6 (number of output filters) x 3 (number of kernels) x 3 x 3 Bias term = 6\n",
        "\n",
        "2. input size : 60×60, 24 channels filter size : 3×3, 48 channels stride : 1 padding : none\n",
        "\n",
        "Answer Output size = 58 x 58 Number of parameters = 48 (number of output filters) x 24 (number of kernels) x 3 x 3 Bias term = 48\n",
        "\n",
        "3. input size : 20×20, 10 channels filter size : 3×3, 20 channels stride : 2 padding : none\n",
        "\n",
        "Answer Output size = 10 x 10 Number of parameters = 20 (number of output filters) x 10 (number of kernels) x 3 x 3 Bias term = 20"
      ]
    }
  ]
}