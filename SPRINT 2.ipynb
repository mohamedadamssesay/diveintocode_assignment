{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scratch on train_test_split\n",
    "def scratch_train_test_split(X, y, train_size=0.8):\n",
    "    arrays = [X,y]\n",
    "    random_state=None\n",
    "    test_size=None\n",
    "    length = len(arrays[0])\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "    p = np.random.permutation(length)\n",
    "\n",
    "    if type(test_size) == int:\n",
    "        index = length - test_size\n",
    "    elif type(test_size) == float:\n",
    "        index = length - np.ceil(length * test_size)\n",
    "    else:\n",
    "        if type(train_size) == int:\n",
    "            index = train_size\n",
    "        elif type(train_size) == float:\n",
    "            index = int(length * train_size)\n",
    "        else:\n",
    "            index = length - np.ceil(length * 0.25)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = [b for a in arrays for b in (a[p][:index], a[p][index:])]\n",
    "    pass\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "df = pd.DataFrame(data=iris.data,columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "# filtering the df to get two target variables we need\n",
    "df_filtered = df[df['target'] > 0] \n",
    "\n",
    "y = df_filtered['target']\n",
    "X = df_filtered.drop(['target'],axis=1)\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = scratch_train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=0)\n",
    "n_samples = 500\n",
    "f0 = [-1, 2]\n",
    "f1 = [2, -1]\n",
    "cov = [[1.0,0.8], [0.8, 1.0]]\n",
    "f0 = np.random.multivariate_normal(f0, cov, int(n_samples/2))\n",
    "f1 = np.random.multivariate_normal(f1, cov, int(n_samples/2))\n",
    "X = np.concatenate((f0, f1))\n",
    "y = np.concatenate((np.ones((int(n_samples/2))), np.ones((int(n_samples/2))) *(-1))).astype(np.int)\n",
    "random_index = np.random.permutation(np.arange(n_samples))\n",
    "X = X[random_index]\n",
    "y = y[random_index]\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = scratch_train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-0.44699 , -2.8073  ],[-1.4621  , -2.4586  ],\n",
    "       [ 0.10645 ,  1.9242  ],[-3.5944  , -4.0112  ],\n",
    "       [-0.9888  ,  4.5718  ],[-3.1625  , -3.9606  ],\n",
    "       [ 0.56421 ,  0.72888 ],[-0.60216 ,  8.4636  ],\n",
    "       [-0.61251 , -0.75345 ],[-0.73535 , -2.2718  ],\n",
    "       [-0.80647 , -2.2135  ],[ 0.86291 ,  2.3946  ],\n",
    "       [-3.1108  ,  0.15394 ],[-2.9362  ,  2.5462  ],\n",
    "       [-0.57242 , -2.9915  ],[ 1.4771  ,  3.4896  ],\n",
    "       [ 0.58619 ,  0.37158 ],[ 0.6017  ,  4.3439  ],\n",
    "       [-2.1086  ,  8.3428  ],[-4.1013  , -4.353   ],\n",
    "       [-1.9948  , -1.3927  ],[ 0.35084 , -0.031994],\n",
    "       [ 0.96765 ,  7.8929  ],[-1.281   , 15.6824  ],\n",
    "       [ 0.96765 , 10.083   ],[ 1.3763  ,  1.3347  ],\n",
    "       [-2.234   , -2.5323  ],[-2.9452  , -1.8219  ],\n",
    "       [ 0.14654 , -0.28733 ],[ 0.5461  ,  5.8245  ],\n",
    "       [-0.65259 ,  9.3444  ],[ 0.59912 ,  5.3524  ],\n",
    "       [ 0.50214 , -0.31818 ],[-3.0603  , -3.6461  ],\n",
    "       [-6.6797  ,  0.67661 ],[-2.353   , -0.72261 ],\n",
    "       [ 1.1319  ,  2.4023  ],[-0.12243 ,  9.0162  ],\n",
    "       [-2.5677  , 13.1779  ],[ 0.057313,  5.4681  ]])\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = scratch_train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First dataset, Accuracy_SDG:  1.0\n",
      "First dataset, Accuracy_SVM:  1.0\n",
      "First dataset, Accuracy_DT:  0.95\n",
      "-------------------\n",
      "Second dataset, Accuracy_SDG:  1.0\n",
      "Second dataset, Accuracy_SVM:  1.0\n",
      "Second dataset, Accuracy_DT:  1.0\n",
      "-------------------\n",
      "third dataset, Accuracy_SDG:  0.625\n",
      "third dataset, Accuracy_SVM:  0.75\n",
      "third dataset, Accuracy_DT:  0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# import classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# creating instances\n",
    "clf_SDG = make_pipeline(StandardScaler(),\n",
    "                     SGDClassifier(max_iter=1000, tol=1e-3,loss=\"log\"))\n",
    "clf_SVM = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf_DT = make_pipeline(StandardScaler(),DecisionTreeClassifier(random_state=0))\n",
    "\n",
    "# for first dataset\n",
    "clf_SDG.fit(X_train_1, y_train_1)\n",
    "pred_1_SDG = clf_SDG.predict(X_test_1)\n",
    "clf_SVM.fit(X_train_1, y_train_1)\n",
    "pred_1_SVM = clf_SVM.predict(X_test_1)\n",
    "clf_DT.fit(X_train_1, y_train_1)\n",
    "pred_1_DT = clf_DT.predict(X_test_1)\n",
    "\n",
    "print(\"First dataset, Accuracy_SDG: \", accuracy_score(y_test_1,pred_1_SDG))\n",
    "print(\"First dataset, Accuracy_SVM: \", accuracy_score(y_test_1,pred_1_SVM))\n",
    "print(\"First dataset, Accuracy_DT: \", accuracy_score(y_test_1,pred_1_DT))\n",
    "\n",
    "# for second dataset\n",
    "clf_SDG.fit(X_train_2, y_train_2)\n",
    "pred_2_SDG = clf_SDG.predict(X_test_2)\n",
    "clf_SVM.fit(X_train_2, y_train_2)\n",
    "pred_2_SVM = clf_SVM.predict(X_test_2)\n",
    "clf_DT.fit(X_train_2, y_train_2)\n",
    "pred_2_DT = clf_DT.predict(X_test_2)\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Second dataset, Accuracy_SDG: \", accuracy_score(y_test_2,pred_2_SDG))\n",
    "print(\"Second dataset, Accuracy_SVM: \", accuracy_score(y_test_2,pred_2_SVM))\n",
    "print(\"Second dataset, Accuracy_DT: \", accuracy_score(y_test_2,pred_2_DT))\n",
    "\n",
    "# for third dataset\n",
    "clf_SDG.fit(X_train_3, y_train_3)\n",
    "pred_3_SDG = clf_SDG.predict(X_test_3)\n",
    "clf_SVM.fit(X_train_3, y_train_3)\n",
    "pred_3_SVM = clf_SVM.predict(X_test_3)\n",
    "clf_DT.fit(X_train_3, y_train_3)\n",
    "pred_3_DT = clf_DT.predict(X_test_3)\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"third dataset, Accuracy_SDG: \", accuracy_score(y_test_3,pred_3_SDG))\n",
    "print(\"third dataset, Accuracy_SVM: \", accuracy_score(y_test_3,pred_3_SVM))\n",
    "print(\"third dataset, Accuracy_DT: \", accuracy_score(y_test_3,pred_3_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id               0\n",
       "MSSubClass       0\n",
       "LotFrontage      0\n",
       "LotArea          0\n",
       "OverallQual      0\n",
       "OverallCond      0\n",
       "YearBuilt        0\n",
       "YearRemodAdd     0\n",
       "MasVnrArea       0\n",
       "BsmtFinSF1       0\n",
       "BsmtFinSF2       0\n",
       "BsmtUnfSF        0\n",
       "TotalBsmtSF      0\n",
       "1stFlrSF         0\n",
       "2ndFlrSF         0\n",
       "LowQualFinSF     0\n",
       "GrLivArea        0\n",
       "BsmtFullBath     0\n",
       "BsmtHalfBath     0\n",
       "FullBath         0\n",
       "HalfBath         0\n",
       "BedroomAbvGr     0\n",
       "KitchenAbvGr     0\n",
       "TotRmsAbvGrd     0\n",
       "Fireplaces       0\n",
       "GarageYrBlt      0\n",
       "GarageCars       0\n",
       "GarageArea       0\n",
       "WoodDeckSF       0\n",
       "OpenPorchSF      0\n",
       "EnclosedPorch    0\n",
       "3SsnPorch        0\n",
       "ScreenPorch      0\n",
       "PoolArea         0\n",
       "MiscVal          0\n",
       "MoSold           0\n",
       "YrSold           0\n",
       "SalePrice        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the dataset\n",
    "df_2 = pd.read_csv('train.csv')\n",
    "\n",
    "# select columns with numbers\n",
    "altered_df2 = df_2.select_dtypes(\"number\")\n",
    "\n",
    "# fill empty cells\n",
    "altered_df2 = altered_df2.fillna(0)\n",
    "\n",
    "# check if there are any remaining empty data\n",
    "altered_df2.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120000 120000 158000 350000 120000 173000 172500 130000 144000 130000\n",
      " 187000 118000 215000 350000 120000 144000 172500 120000 172500 130000\n",
      " 158000  94000 120000 172500 173000 172500 170000 130000 130000 350000\n",
      " 172500 130000 215000 120000 130000 158000 158000 130000 120000 120000\n",
      " 130000 172500 187000 350000 172500 120000 158000 130000 350000 158000\n",
      " 158000 130000 172500 120000 172500 130000 263435 120000 130000 172500\n",
      " 158000 120000 130000  84500 295000 263435 350000 120000 187000 130000\n",
      " 173000 170000 130000 170000 144000 130000 120000 130000 158000 172500\n",
      " 120000 172500 130000 350000 130000 120000 144000 130000 130000 263435\n",
      " 215000 170000 172500 173000 120000 118000 158000 120000 120000 130000\n",
      " 130000 215000 170000 187000 120000 130000 130000 158000 158000 118000\n",
      " 144000 130000 187000 120000 130000 170000 172500 350000 350000 172500\n",
      " 130000 158000 172500 130000 215000 172500 130000 350000 184750 130000\n",
      " 160000 130000 170000 130000 170000 130000 350000 350000 350000 187000\n",
      " 130000 130000 350000 130000 130000 187000 120000 350000 120000 120000\n",
      " 172500 120000 130000 130000 130000 120000 120000 187000 158000 130000\n",
      " 350000 350000 130000 130000 144000 173000 130000 172500 172500 120000\n",
      " 144000 215000 120000 172500 130000 172500 215000 158000 130000 172500\n",
      " 172500 120000 130000 130000 130000 130000 130000 130000 130000 172500\n",
      " 187000 120000 120000 130000 172500 130000 120000 130000 120000 130000\n",
      " 130000 130000 130000 120000 130000 158000 130000 130000 172500 130000\n",
      " 130000 130000 173000 130000 173000 187000 144000 130000 120000 172500\n",
      " 130000 170000 350000 170000 173000 263435 215200 172500 130000 172500\n",
      " 130000 120000 172500 197000 130000 130000 172500 130000 158000 350000\n",
      " 350000 130000 130000 130000 172500 130000 130000 120000 130000 130000\n",
      " 130000 187000 172500 130000 130000 130000 350000 144000 187000 144000\n",
      " 130000 130000 120000 158000 130000 172500 350000 130000 120000 130000\n",
      " 172500 120000 118000 130000 120000 350000 130000 172500 187000 172500\n",
      " 144000 350000 120000 187000 350000 130000 350000 158000 172500 130000\n",
      " 120000 120000]\n"
     ]
    }
   ],
   "source": [
    "X = altered_df2[['GrLivArea','YearBuilt']]\n",
    "y = altered_df2['SalePrice']\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = scratch_train_test_split(X, y)\n",
    "\n",
    "# creating an instance without the loss=log parameter\n",
    "clf_SDG = make_pipeline(StandardScaler(),\n",
    "                     SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "\n",
    "clf_SDG.fit(X_train_4, y_train_4)\n",
    "pred_4_SDG = clf_SDG.predict(X_test_4)\n",
    "\n",
    "print(pred_4_SDG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
