{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBK2UwrZbCFG"
   },
   "source": [
    "# **[Problem 1] Confirmation of competition contents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzGhOX3TbzGG"
   },
   "source": [
    "* **What to learn and what to predic?**\n",
    "\n",
    "This prompt us to learn about the **Transaction Information** relative to the ***clients*** and to be able to Predict the **Payment Abilities**  of the ***clients***. This is relative to the probability for the TARGET variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0vmNZsLdT10"
   },
   "source": [
    "* **What kind of file to create and submit to Kaggle?**\n",
    "\n",
    "For each **SK_ID_CURR** in the test set, you must be able to predict a probability for the **TARGET** variable. \n",
    "\n",
    "**NOTE:** The file should contain a header and have the following format: **SK_ID_CURR,TARGET**\n",
    "* 100001,0.1\n",
    "* 100005,0.9\n",
    "* 100013,0.2 etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlY8_UaFeiYs"
   },
   "source": [
    "* **What kind of index value will be used to evaluate the submissions?**\n",
    "\n",
    "The submissions are evaluated on area under the **Receiver Operating Characteristic(ROC) curve** between the predicted probability and the observed target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xke3pRcAfR_z"
   },
   "source": [
    "# **[Problem 2] Learning and verification**\n",
    "\n",
    "* We import all needed libraries\n",
    "* Load the Data (application_train.csv)\n",
    "* Delete the Null data(empty data)\n",
    "* Extract or separate them into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "z2_I2Uu0fgU2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "df = pd.read_csv('application_train.csv')\n",
    "\n",
    "\n",
    "cleaned_df = df.dropna()\n",
    "\n",
    "\n",
    "X = cleaned_df.loc[:,[\"AMT_INCOME_TOTAL\",\"AMT_CREDIT\",\"AMT_ANNUITY\"]]\n",
    "y = cleaned_df['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "9xVTdL0-gwTI",
    "outputId": "6e571638-5643-4f4d-d25d-7e1e3161f5d4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>NAME_TYPE_SUITE</th>\n",
       "      <th>NAME_INCOME_TYPE</th>\n",
       "      <th>NAME_EDUCATION_TYPE</th>\n",
       "      <th>NAME_FAMILY_STATUS</th>\n",
       "      <th>NAME_HOUSING_TYPE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_PUBLISH</th>\n",
       "      <th>OWN_CAR_AGE</th>\n",
       "      <th>FLAG_MOBIL</th>\n",
       "      <th>FLAG_EMP_PHONE</th>\n",
       "      <th>FLAG_WORK_PHONE</th>\n",
       "      <th>FLAG_CONT_MOBILE</th>\n",
       "      <th>FLAG_PHONE</th>\n",
       "      <th>FLAG_EMAIL</th>\n",
       "      <th>OCCUPATION_TYPE</th>\n",
       "      <th>CNT_FAM_MEMBERS</th>\n",
       "      <th>REGION_RATING_CLIENT</th>\n",
       "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
       "      <th>WEEKDAY_APPR_PROCESS_START</th>\n",
       "      <th>HOUR_APPR_PROCESS_START</th>\n",
       "      <th>REG_REGION_NOT_LIVE_REGION</th>\n",
       "      <th>REG_REGION_NOT_WORK_REGION</th>\n",
       "      <th>LIVE_REGION_NOT_WORK_REGION</th>\n",
       "      <th>REG_CITY_NOT_LIVE_CITY</th>\n",
       "      <th>REG_CITY_NOT_WORK_CITY</th>\n",
       "      <th>LIVE_CITY_NOT_WORK_CITY</th>\n",
       "      <th>...</th>\n",
       "      <th>LIVINGAPARTMENTS_MEDI</th>\n",
       "      <th>LIVINGAREA_MEDI</th>\n",
       "      <th>NONLIVINGAPARTMENTS_MEDI</th>\n",
       "      <th>NONLIVINGAREA_MEDI</th>\n",
       "      <th>FONDKAPREMONT_MODE</th>\n",
       "      <th>HOUSETYPE_MODE</th>\n",
       "      <th>TOTALAREA_MODE</th>\n",
       "      <th>WALLSMATERIAL_MODE</th>\n",
       "      <th>EMERGENCYSTATE_MODE</th>\n",
       "      <th>OBS_30_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DEF_30_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>OBS_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DEF_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DAYS_LAST_PHONE_CHANGE</th>\n",
       "      <th>FLAG_DOCUMENT_2</th>\n",
       "      <th>FLAG_DOCUMENT_3</th>\n",
       "      <th>FLAG_DOCUMENT_4</th>\n",
       "      <th>FLAG_DOCUMENT_5</th>\n",
       "      <th>FLAG_DOCUMENT_6</th>\n",
       "      <th>FLAG_DOCUMENT_7</th>\n",
       "      <th>FLAG_DOCUMENT_8</th>\n",
       "      <th>FLAG_DOCUMENT_9</th>\n",
       "      <th>FLAG_DOCUMENT_10</th>\n",
       "      <th>FLAG_DOCUMENT_11</th>\n",
       "      <th>FLAG_DOCUMENT_12</th>\n",
       "      <th>FLAG_DOCUMENT_13</th>\n",
       "      <th>FLAG_DOCUMENT_14</th>\n",
       "      <th>FLAG_DOCUMENT_15</th>\n",
       "      <th>FLAG_DOCUMENT_16</th>\n",
       "      <th>FLAG_DOCUMENT_17</th>\n",
       "      <th>FLAG_DOCUMENT_18</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>-9461</td>\n",
       "      <td>-637</td>\n",
       "      <td>-3648.0</td>\n",
       "      <td>-2120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>reg oper account</td>\n",
       "      <td>block of flats</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>Stone, brick</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>Family</td>\n",
       "      <td>State servant</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>-16765</td>\n",
       "      <td>-1188</td>\n",
       "      <td>-1186.0</td>\n",
       "      <td>-291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Core staff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>0.0558</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.01</td>\n",
       "      <td>reg oper account</td>\n",
       "      <td>block of flats</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>Block</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-828.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>-19046</td>\n",
       "      <td>-225</td>\n",
       "      <td>-4260.0</td>\n",
       "      <td>-2531</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-815.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>-19005</td>\n",
       "      <td>-3039</td>\n",
       "      <td>-9833.0</td>\n",
       "      <td>-2437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>WEDNESDAY</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-617.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>-19932</td>\n",
       "      <td>-3038</td>\n",
       "      <td>-4311.0</td>\n",
       "      <td>-3458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Core staff</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>THURSDAY</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1106.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET  ... AMT_REQ_CREDIT_BUREAU_QRT AMT_REQ_CREDIT_BUREAU_YEAR\n",
       "0      100002       1  ...                       0.0                        1.0\n",
       "1      100003       0  ...                       0.0                        0.0\n",
       "2      100004       0  ...                       0.0                        0.0\n",
       "3      100006       0  ...                       NaN                        NaN\n",
       "4      100007       0  ...                       0.0                        0.0\n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()#loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ld4pNJSCg0_u"
   },
   "source": [
    "**We are spliting the data into traning and testing data (sklearn)**\n",
    "\n",
    "* Standardizing the data\n",
    "* fit and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZErLcOSWhEXu",
    "outputId": "3af5cf3f-b60e-4a91-de22-cd32ed566856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0567142335416132\n",
      "ROC 0.5245983935742973\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_trans = scaler.transform(X_train)\n",
    "X_test_trans = scaler.transform(X_test)\n",
    "\n",
    "reg = LinearRegression().fit(X_train_trans, y_train)\n",
    "\n",
    "reg_pred = reg.predict(X_test_trans)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_true=y_test, y_pred=reg_pred))\n",
    "print(\"ROC\", roc_auc_score(y_test,reg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3cn0n_8hP4X"
   },
   "source": [
    "**MSE** is very low which is a good indication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I03tmQoxhXaZ"
   },
   "source": [
    "## **[Problem 3] Estimation on test data**\n",
    "\n",
    "* Performing the estimation on the test data (application_test.csv ) and submit it to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jo025B91he7F"
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7HTi0-IhvdD"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('application_test.csv')\n",
    "\n",
    "test_cleaned_df = test_df.dropna(axis=0)\n",
    "\n",
    "test_X = test_cleaned_df.loc[:,[\"AMT_INCOME_TOTAL\",\"AMT_CREDIT\",\"AMT_ANNUITY\"]]\n",
    "\n",
    "test_scaler = StandardScaler()\n",
    "test_X_test_trans = scaler.fit_transform(test_X)\n",
    "\n",
    "test_reg_pred = reg.predict(test_X_test_trans)\n",
    "#submitting to kaggle\n",
    "kgl_submission = pd.concat([test_df['SK_ID_CURR'], pd.Series(test_reg_pred, name='TARGET')], axis=1)\n",
    "kgl_submission = kgl_submission.fillna(0)\n",
    "kgl_submission.at[648,'TARGET']= 0\n",
    "kgl_submission.shape\n",
    "kgl_submission.to_csv('kggl_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVZ2NZSth0cI"
   },
   "source": [
    "## **[Problem 4] Feature Engineering**\n",
    "\n",
    "To improve accuracy, perform Feature Engineering from the following perspectives.\n",
    "* Which feature to use?\n",
    "* How to preprocess?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blSSW1IEiT0I"
   },
   "source": [
    "**Pattern 1 of training and validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Cguh7pJia5m"
   },
   "outputs": [],
   "source": [
    "# imputation\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "imp_mean = SimpleImputer(strategy='mean')\n",
    "\n",
    "# deleting the missing values\n",
    "imp_X = imp_mean.fit_transform(X)\n",
    "\n",
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_imp_X = enc.fit_transform(imp_X).toarray()\n",
    "\n",
    "# splitting the data into training and testing data using train_test_split from sklearn\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(enc_imp_X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# we standardized the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_1)\n",
    "X_train_trans_1 = scaler.transform(X_train_1)\n",
    "X_test_trans_1 = scaler.transform(X_test_1)\n",
    "\n",
    "# fitting the data\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier(random_state=5)\n",
    "lgb = lgbm.fit(X_train_trans_1, y_train_1)\n",
    "\n",
    "# predicting\n",
    "reg_pred_1 = lgb.predict(X_test_trans_1)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test_1,reg_pred_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSQUxujfim34"
   },
   "source": [
    "**Pattern 2 of training and validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeHtRlhsiqYn"
   },
   "outputs": [],
   "source": [
    "imp_median = SimpleImputer(strategy='median')\n",
    "\n",
    "# deleting the missing values\n",
    "imp_X_1 = imp_median.fit_transform(X)\n",
    "\n",
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc_1 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_imp_X_1 = enc.fit_transform(imp_X_1).toarray()\n",
    "\n",
    "# splitting the data into training and testing data using train_test_split from sklearn\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(enc_imp_X_1, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# we standardized the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_2)\n",
    "X_train_trans_2 = scaler.transform(X_train_2)\n",
    "X_test_trans_2 = scaler.transform(X_test_2)\n",
    "\n",
    "# fitting the data\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm_1 = LGBMClassifier(random_state=5)\n",
    "lgb_1 = lgbm_1.fit(X_train_trans_2, y_train_2)\n",
    "\n",
    "# predicting\n",
    "reg_pred_2 = lgb_1.predict(X_test_trans_2)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test_2,reg_pred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SErQCsOxiuPn"
   },
   "source": [
    "**Pattern 3 of training and validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8crZfBti1Lr"
   },
   "outputs": [],
   "source": [
    "imp_mf = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# deleting the missing values\n",
    "imp_X_2 = imp_mf.fit_transform(X)\n",
    "\n",
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc_2 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_imp_X_2 = enc.fit_transform(imp_X_2).toarray()\n",
    "\n",
    "# splitting the data into training and testing data using train_test_split from sklearn\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(enc_imp_X_2, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# we standardized the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_3)\n",
    "X_train_trans_3 = scaler.transform(X_train_3)\n",
    "X_test_trans_3 = scaler.transform(X_test_3)\n",
    "\n",
    "# fitting the data\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm_2 = LGBMClassifier(random_state=5)\n",
    "lgb_2 = lgbm_2.fit(X_train_trans_3, y_train_3)\n",
    "\n",
    "# predicting\n",
    "reg_pred_3 = lgb_2.predict(X_test_trans_3)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test_3,reg_pred_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZuoBRMGi4kj"
   },
   "source": [
    "**Pattern 4 of training and validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pJBa9Iti-r9"
   },
   "outputs": [],
   "source": [
    "imp_cnst = SimpleImputer(strategy='constant')\n",
    "\n",
    "# deleting the missing values\n",
    "imp_X_3 = imp_cnst.fit_transform(X)\n",
    "\n",
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc_3 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_imp_X_3 = enc.fit_transform(imp_X_3).toarray()\n",
    "\n",
    "# splitting the data into training and testing data using train_test_split from sklearn\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(enc_imp_X_3, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# we standardized the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_4)\n",
    "X_train_trans_4 = scaler.transform(X_train_4)\n",
    "X_test_trans_4 = scaler.transform(X_test_4)\n",
    "\n",
    "# fitting the data\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm_3 = LGBMClassifier(random_state=5)\n",
    "lgb_3 = lgbm_3.fit(X_train_trans_4, y_train_4)\n",
    "\n",
    "# predicting\n",
    "reg_pred_4 = lgb_3.predict(X_test_trans_4)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test_4,reg_pred_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTcEua54jEdx"
   },
   "source": [
    "**Pattern 5 of training and validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4V2D1kdMjH17"
   },
   "outputs": [],
   "source": [
    "imp_cnst = SimpleImputer(strategy='constant')\n",
    "\n",
    "# deleting the missing values\n",
    "imp_X_4 = imp_cnst.fit_transform(X)\n",
    "\n",
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc_4 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_imp_X_4 = enc.fit_transform(imp_X_4).toarray()\n",
    "\n",
    "# splitting the data into training and testing data using train_test_split from sklearn\n",
    "X_train_5, X_test_5, y_train_5, y_test_5 = train_test_split(enc_imp_X_4, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# we standardizied data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_5)\n",
    "X_train_trans_5 = scaler.transform(X_train_5)\n",
    "X_test_trans_5 = scaler.transform(X_test_5)\n",
    "\n",
    "# fitting the data\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm_4 = LGBMClassifier(random_state=5)\n",
    "lgb_4 = lgbm_4.fit(X_train_trans_5, y_train_5)\n",
    "\n",
    "# predicting\n",
    "reg_pred_5 = lgb_4.predict(X_test_trans_5)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test_5,reg_pred_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy0qbbcKjV82"
   },
   "source": [
    "* **The feature and preprocess used is** IMPUTAION and HOT ENCODING **techniques**.\n",
    "* **These techniques are more useful, and from what is noticed, when using simple imputer the accuracy will still be high and they also stays constant.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week4.4 - Introductory Machine Learning Class assignment Learning Credit Information.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
