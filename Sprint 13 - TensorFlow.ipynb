{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sprint 13 - TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9qUGn75fYBV"
      },
      "source": [
        "# [Problem 1] Looking back on the scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgJ745VxgXQB"
      },
      "source": [
        "Looking back at your scratching so far, list what you needed to implement deep learning.\n",
        "\n",
        "##Answer.\n",
        "Initialization and update of weights and biases\n",
        "Looping through epochs.\n",
        "Output through activation function.\n",
        "Repeat forward and back propagation.\n",
        "\n",
        "Preparing the data set\n",
        "We will use the Iris dataset that we have used before. In the following sample code, it is assumed that Iris.csv is in the same hierarchy.\n",
        "\n",
        "Iris Species\n",
        "\n",
        "The objective variable is Species, but only the following two species are used out of the three types.\n",
        "\n",
        "Iris-versicolor\n",
        "Iris-virginica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAXEyIk9gdNJ"
      },
      "source": [
        "# [Problem 2] Consider the correspondence between scratch and TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzKZzE3ugp8l"
      },
      "source": [
        "\"\"\"\n",
        "Binary classification of Iris dataset using a neural network implemented in TensorFlow.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "# Loading a Data Set\n",
        "dataset_path =\"datasets_19_420_Iris.csv\" #read csv\n",
        "df = pd.read_csv(dataset_path) # dataframe creation\n",
        "\n",
        "# Extract conditions from data frame\n",
        "# Create X and y\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "# Convert labels to numbers.\n",
        "# And while we're at it, let's make y two-dimensional.\n",
        "y[y=='Iris-versicolor'] = 0\n",
        "y[y=='Iris-virginica'] = 1\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# Further split into train and val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# Mini Batch Class\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get the mini-batch\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray of the following form, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : ndarray of the following form, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      batch size\n",
        "    seed : int\n",
        "      Seed of random number in NumPy\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "    \n",
        "\n",
        "# Hyperparameter settings\n",
        "learning_rate = 0.01 # learning rate\n",
        "batch_size = 10 # batch size\n",
        "num_epochs = 10 # number of epochs\n",
        "n_hidden1 = 50 # output size of hidden layer\n",
        "n_hidden2 = 100 # output size of hidden layer 2\n",
        "n_input = X_train.shape[1] # number of input columns, number of features\n",
        "n_samples = X_train.shape[0] # Number of input rows, number of data\n",
        "n_classes = 1 # Number of classes?\n",
        "\n",
        "# Determine the form of the argument to pass to the computational graph.\n",
        "# https://note.nkmk.me/python-tensorflow-constant-variable-placeholder/\n",
        "X = tf.placeholder(\"float\", [None, n_input]) # Placeholder is a container where data is stored. The data is undecided and the graph is constructed, and the concrete values are given at runtime.\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train's mini-batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "# Is this the part of the computation graph? Network creation function. In Scratch, it's SimpleConvnet.\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    A simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    # Declare weights and biases.\n",
        "    # Declare variables in tf.Variable. The contents are randomly created with tf.random_normal, just like np.random.normal.\n",
        "\n",
        "Translated with www.DeepL.com/Translator (free version)\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    \n",
        "    # Layer iterator. tf.matmul is a matrix product, like np.dot. It seems to behave slightly differently, so check.\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output\n",
        "\n",
        "\n",
        "# Load the network structure                               \n",
        "logits = example_net(X)\n",
        "# Objective function\n",
        "# sigmoid_cross_entropy_with_logits calculates the cross entropy error through the sigmoid function.\n",
        "# Inside is z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x)) x=logits, z=labels\n",
        "# label minus log(sigmoid(x)) + (1 - label) minus log(1 - sigmoid(x))\n",
        "# if less than or equal to 0, we get 0, so to summarize, max(x, 0) - x * z + log(1 + exp(-abs(x)))\n",
        "# reduce_mean is the same as np.mean, if axis=None, a single scalar will be returned\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# Optimization method\n",
        "# Instantiate\n",
        "AdamOptimizer(learning_rate=learning_rate)\n",
        "# Called by minimize()\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Estimation results\n",
        "# tf.equal compares and returns True if they are the same, False if they are different as an array\n",
        "# sign is converted to sine wave, not sure what -0.5 means.\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "# index value calculation\n",
        "# cast changes the type of the tensor, like np.astype?\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Instance to initialize the variable.\n",
        "# This is just a shortcut for `variables_initializer(global_variables())`.\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Run the computational graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwxckNpQtS_C"
      },
      "source": [
        "### Answers and notes\n",
        "Tensorflow declares variables and such first. Isn't it an object type?\\\n",
        "It's not a surprise that it runs in C++. If the entrance is written in Python but the content is written in C++, it is better to assume that the basics are in the C system.\\\n",
        "Correlation with home-made CNNs\n",
        "\n",
        "Activation function → through a layered iterator, done with tf.nn.relu(), you can change the relu part.\\\n",
        "Error function → implemented in tf.nn.sigmoid_cross_entropy_with_logits\\\n",
        "Full coupling layer → implemented in iterator layer, with network structure in logits, forward propagation when loss_op runs, loss function and gradient update in train_op. AdamOptimizer is used in this model.\\\n",
        "Epoch and mini-batch → Same here. Mini-batch functions and epochs are just normal for statements.\\\n",
        "Optimizer → Instantiate and call with minimize()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlNGnevNteDC"
      },
      "source": [
        "3, Application to other data sets\\\n",
        "There are several small datasets that we have been working with so far. Rewrite the above sample code to create a neural network that will train and estimate on these.\n",
        "\n",
        "Iris (using all three objective variables)\n",
        "House Prices\\\n",
        "Use all three types of data sets: train, val, and test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e4jMeujt16U"
      },
      "source": [
        "# [Problem 3] Create a model of Iris using all three types of objective variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KuHa1JHuCEw"
      },
      "source": [
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANmnz03suEih"
      },
      "source": [
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtBl3CXpuOFm"
      },
      "source": [
        "import sys\n",
        "\"\"\"\n",
        "Tri-level classification of Iris dataset using a neural network implemented in TensorFlow.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "# Load the dataset\n",
        "dataset_path =\"datasets_19_420_Iris.csv\" # read csv\n",
        "df = pd.read_csv(dataset_path) # dataframe it\n",
        "\n",
        "# Extract conditions from dataframe\n",
        "# Create X and y\n",
        "#df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "#display(y)\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "# Convert the labels to one-hot-vectors.\n",
        "# And while we're at it, make y two-dimensional\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y = enc.fit_transform(y[:, np.newaxis])\n",
        "#print(y.shape)\n",
        "#display(y)\n",
        "\n",
        "# split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# Further split into train and val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "#print(X_train.shape)\n",
        "\n",
        "# MiniBatch class\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get the mini-batch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray of the following form, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : ndarray of the following form, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      batch size\n",
        "    seed : int\n",
        "      Seed of random number in NumPy\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "    \n",
        "\n",
        "# Hyperparameter settings\n",
        "learning_rate = 0.01 # learning rate\n",
        "batch_size = 10 # batch size\n",
        "num_epochs = 10 # number of epochs\n",
        "n_hidden1 = 50 # output size of hidden layer\n",
        "n_hidden2 = 100 # output size of hidden layer 2\n",
        "n_input = X_train.shape[1] # number of input columns, number of features\n",
        "n_samples = X_train.shape[0] # Number of input rows, number of data\n",
        "n_classes = 3 # Number of classes. If you are doing binary classification and only need one column, 1 is fine, but if you are doing one-hot classification, increase the number. In this case, 3.\n",
        "\n",
        "# Determine the form of the argument to pass to the computational graph.\n",
        "# https://note.nkmk.me/python-tensorflow-constant-variable-placeholder/\n",
        "X = tf.placeholder(\"float\", [None, n_input]) # The placeholder is a container for the data. The data is undecided and the graph is constructed, and the concrete values are given at runtime.\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train's mini-batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "# Is this the part of the computation graph? Network creation function. In Scratch, it's SimpleConvnet.\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    A simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    # Declare weights and biases.\n",
        "    # Declare variables in tf.Variable. The contents are randomly created with tf.random_normal, just like np.random.normal.\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1], seed=128)),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2], seed=128)),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes], seed=128))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1], seed=128)),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2], seed=128)),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes], seed=128))\n",
        "    }\n",
        "    \n",
        "    # Layer iterator. tf.matmul is a matrix product, like np.dot. It seems to behave slightly differently, so check.\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    #print(layer_1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    #print(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    #print(layer_2)\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    #print(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    #print(layer_output)\n",
        "    return layer_output\n",
        "    \n",
        "\n",
        "# Load the network structure                               \n",
        "logits = example_net(X)\n",
        "# Objective function\n",
        "# sigmoid_cross_entropy_with_logits calculates the cross entropy error through the sigmoid function.\n",
        "# Inside is z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x)) x=logits, z=labels\n",
        "# label minus log(sigmoid(x)) + (1 - label) minus log(1 - sigmoid(x))\n",
        "# if less than or equal to 0, we get 0, so to summarize, max(x, 0) - x * z + log(1 + exp(-abs(x)))\n",
        "# reduce_mean is the same as np.mean, if axis=None, a single scalar will be returned\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# Optimization method\n",
        "# Instantiate\n",
        "AdamOptimizer(learning_rate=learning_rate)\n",
        "# Called by minimize()\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Estimation results\n",
        "# tf.equal compares and returns True if they are the same, False if they are different as an array\n",
        "# sign is converted to sine wave, not sure what -0.5 means.\n",
        "softmax_out = tf.nn.softmax(logits, axis=1)\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(softmax_out, 1))\n",
        "#correct_pred = tf.equal(tf.argmax(Y), tf.argmax(softmax_out))\n",
        "# index value calculation\n",
        "# cast changes the type of the tensor, like np.astype?\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Instance to initialize the variable.\n",
        "# This is just a shortcut for `variables_initializer(global_variables())`.\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# This is just a shortcut for `variables_initializer(global_variables()` init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# Run the computational graph\n",
        "with tf.Session() as sess:\n",
        "    Sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            #print(sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y}))\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            #print(sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y}))\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7St2bqeuvyG"
      },
      "source": [
        "# [Problem 4] Creating a model of House Prices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gO7-fcruxKl"
      },
      "source": [
        "#import os\n",
        "#import random\n",
        "#os.environ['PYTHONHASHSEED'] = \"0\"\n",
        "#np.random.seed(0)\n",
        "#random.seed(0)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\"\"\"\n",
        "Regression classification of the HousePrice dataset using a neural network implemented in TensorFlow.\n",
        "\"\"\"\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path =\"train.csv\" # read csv\n",
        "df = pd.read_csv(dataset_path) # dataframe it\n",
        "\n",
        "# Extract conditions from dataframe\n",
        "# Create X and y\n",
        "#df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df.loc[:,[\"SalePrice\"]]\n",
        "#display(y)\n",
        "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "#print(y.shape)\n",
        "#print(X.shape)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "# Logarithmically transform the objective variable\n",
        "y = np.log1p(y)\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# Further split into train and val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "#print(X_train.shape)\n",
        "\n",
        "#plt.scatter(X_test[:,0],y_test)\n",
        "#plt.show()\n",
        "\n",
        "# Set the hyperparameters\n",
        "learning_rate = 0.01 # learning rate\n",
        "batch_size = 10 # batch size\n",
        "num_epochs = 10 # number of epochs\n",
        "n_hidden1 = 50 # output size of hidden layer\n",
        "n_hidden2 = 100 # output size of hidden layer 2\n",
        "n_input = X_train.shape[1] # number of input columns, number of features\n",
        "n_samples = X_train.shape[0] # Number of input rows, number of data\n",
        "n_classes = 1 # Number of classes. If you are doing binary classification and only need one column, 1 is fine, but if you are doing one-hot classification, increase the number. In this case, 3.\n",
        "\n",
        "# Determine the form of the argument to pass to the computational graph.\n",
        "# https://note.nkmk.me/python-tensorflow-constant-variable-placeholder/\n",
        "X = tf.placeholder(\"float\", [None, n_input]) # The placeholder is a container for the data. The data is undecided and the graph is constructed, and the concrete values are given at runtime.\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train's mini-batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "# Is this the part of the computation graph? Network creation function. In Scratch, it's SimpleConvnet.\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    A simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    # Declare weights and biases.\n",
        "    # Declare variables in tf.Variable. The contents are randomly created with tf.random_normal, just like np.random.normal.\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1], seed=0)),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2], seed=0)),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes], seed=0))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1], seed=0)),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2], seed=0)),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes], seed=0))\n",
        "    }\n",
        "    \n",
        "    # layered iterator. tf.matmul is a matrix product, like np.dot. It seems to behave slightly differently, so check.\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    #print(layer_1)\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    #print(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    #print(layer_2)\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    #print(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    #print(layer_output)\n",
        "    return layer_output\n",
        "\n",
        "\n",
        "# Load the network structure                               \n",
        "logits = example_net(X)\n",
        "# objective function\n",
        "#loss_op = tf.reduce_mean(tf.squared_difference(Y, logits))\n",
        "loss_op = tf.reduce_mean(tf.square(logits - Y))\n",
        "# Optimization method\n",
        "# Instantiate\n",
        "AdamOptimizer(learning_rate=learning_rate)\n",
        "RMSPropOptimizer(learning_rate=learning_rate) #optimizer = tf.train.\n",
        "# call in minimize()\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Estimation results\n",
        "#correct_pred = logits\n",
        "# index value calculation\n",
        "# cast changes the type of the tensor, like np.astype?\n",
        "#accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "accuracy = tf.reduce_mean(tf.squared_difference(Y, logits))\n",
        "\n",
        "# Instance to initialize variable.\n",
        "# This is just a shortcut for `variables_initializer(global_variables())`.\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# This is just a shortcut for `variables_initializer(global_variables()` init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "\n",
        "# Run the computational graph\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    Sess.run(init)    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            #print(sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y}))\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            #print(sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y}))\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= batch_size\n",
        "        total_acc /= batch_size\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, MSE : {:.3f}, val_MSE : {:.3f}\".format(epoch, total_loss, val_loss, total_acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_MSE : {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4SvfwlWvIzP"
      },
      "source": [
        "# [Problem 5] Creating a MNIST model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPmSpjTxvNJu"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(-1, im_rows, im_cols, im_color)\n",
        "X_train = X_train.astype('float64') / 255\n",
        "X_test = X_test.reshape(-1, im_rows, im_cols, im_color)\n",
        "X_test = X_test.astype('float64') / 255\n",
        "print(X_train.max()) # 1.0\n",
        "print(X_train.min()) # 0.0\n",
        "print(X_train[0].dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZDc3Z3JvP3m"
      },
      "source": [
        "# Convert label to one-hot-vector.\n",
        "# and make y two-dimensional.\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test = enc.fit_transform(y_test[:, np.newaxis])\n",
        "print(y_train_one_hot.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Deh29cVKvTW0"
      },
      "source": [
        "# Further split into train and val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKbnMCinvc0g"
      },
      "source": [
        "### [Answer] Self-made function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt2wmVX6vwRo"
      },
      "source": [
        "\"\"\"\n",
        "Using a neural network implemented in TensorFlow to classify MNIST datasets as multi-level\n",
        "\"\"\"\n",
        "\n",
        "# Set the hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "stride = 1\n",
        "pad = 'VALID'\n",
        "ksize = [3, 3]\n",
        "# Determine the form of arguments to be passed to the computation graph.\n",
        "X = tf.placeholder(\"float\", [None, X_train.shape[1], X_train.shape[2], X_train.shape[3]])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "# train's mini-batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    A simple three-layer neural network\n",
        "    \"\"\"\n",
        "    # Declare weights and biases\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([5, 5, 1, 4])),  # H, W, C, F\n",
        "        'w2': tf.Variable(tf.random_normal([3, 3, 4, 16])),\n",
        "        'w3': tf.Variable(tf.random_normal([64, 32])),\n",
        "        'w4': tf.Variable(tf.random_normal([32, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([1, 1, 1, 4])),\n",
        "        'b2': tf.Variable(tf.random_normal([1, 1, 1, 16])),\n",
        "        'b3': tf.Variable(tf.random_normal([32])),\n",
        "        'b4': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.nn.conv2d(x, weights['w1'], stride, pad) + biases['b1']\n",
        "    layer_1 = tf.nn.max_pool2d(layer_1, ksize, ksize, pad)\n",
        "    layer_2 = tf.nn.conv2d(layer_1, weights['w2'], stride, pad) + biases['b2']\n",
        "    layer_2 = tf.nn.max_pool2d(layer_2, ksize, ksize, pad)\n",
        "    layer_2 = tf.layers.Flatten()(layer_2)\n",
        "    layer_3 = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
        "    layer_3 = tf.nn.relu(layer_3)\n",
        "    layer_output = tf.matmul(layer_3, weights['w4']) + biases['b4'] # tf.add and + are equivalent\n",
        "    return layer_output\n",
        "\n",
        "# Load the network structure                               \n",
        "logits = example_net(X)\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits), axis=0)\n",
        "# optimization method\n",
        "AdamOptimizer(learning_rate=learning_rate) optimizer = tf.train.\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# Estimation results\n",
        "correct_pred = tf.equal(tf.argmax(Y, axis=1), tf.argmax(tf.nn.softmax(logits), axis=1))\n",
        "# correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.nn.softmax(logits) - 0.5))\n",
        "# index value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "# Initialize variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Run the computational graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            _, loss, acc = sess.run([train_op, loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
        "    print(sess.run(tf.argmax(tf.nn.softmax(logits), 1), feed_dict={X: X_val, Y: y_val}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijFs37_Qv3pM"
      },
      "source": [
        "###[keras version]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8u58RVVv-MD"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.datasets import mnist\n",
        "\n",
        "\n",
        "im_rows, im_cols = 28, 28\n",
        "im_color = 1\n",
        "in_shape = (im_rows, im_cols, im_color)\n",
        "out_size = 10\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, im_rows, im_cols, im_color)\n",
        "X_train = X_train.astype('float64') / 255\n",
        "X_test = X_test.reshape(-1, im_rows, im_cols, im_color)\n",
        "X_test = X_test.astype('float64') / 255\n",
        "\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train.astype('float64'), 10)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test.astype('float64'), 10)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMxfqTOawBcm"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Early stopping setting\n",
        "# Stop when the element set as monitor falls below the patience count and min_data of change\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.0, patience=2)\n",
        "\n",
        "\n",
        "# Model building\n",
        "model = Sequential() # Instance\n",
        "model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=in_shape)) # 1st layer, convolution\n",
        "model.add(Conv2D(64, (3,3), activation='relu')) # 2nd layer, convolution\n",
        "model.add(MaxPooling2D(pool_size=(2,2))) # 3rd layer, pooling layer\n",
        "model.add(Dropout(0.25)) # Dropout, set percentage of values to 0 (forget)\n",
        "model.add(Flatten()) # flatten, smoothing\n",
        "model.add(Dense(128, activation='relu')) # All coupled layers\n",
        "model.add(Dropout(0.5)) # dropout\n",
        "model.add(Dense(out_size, activation='softmax')) # All coupled layers, final layer\n",
        "\n",
        "#Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
        "\n",
        "# training\n",
        "hist = model.fit(X_train, y_train, batch_size=100, epochs=10, verbose=1, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# Output the score tested on the test data\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"accuracy=\", score[1], 'loss=', score[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET__gtqkwFw1"
      },
      "source": [
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.title('Accuracy')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('Loss')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}